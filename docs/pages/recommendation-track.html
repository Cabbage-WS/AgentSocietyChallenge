<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Recommendation Track - WWW'25 AgentSociety Challenge</title>
    <link rel="stylesheet" href="../assets/css/style.css">
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
</head>
<body>
    <nav class="sidebar">
        <div class="sidebar-logo">
            <img src="../assets/images/ASC.jpg" alt="Logo">
        </div>
        <ul class="nav-links">
            <li><a href="overview.html">Overview</a></li>
            <li><a href="timeline.html">Timeline</a></li>
            <li><a href="participation.html">Participation</a></li>
            <li><a href="rules.html">Rules</a></li>
            <li><a href="behavior-track.html">User Behavior Modeling</a></li>
            <li><a href="recommendation-track.html" class="active">Recommendation Track</a></li>
            <li><a href="submission.html">Submission</a></li>
        </ul>
    </nav>

    <main class="main-content">
        <section class="section" id="content"></section>
    </main>

    <script>
        const markdownContent = `## Recommendation Track
### Introduction
In this track, participants will develop an LLM agent that acts as a recommendation assistant. Participants are expected to design recommendation agents capable of handling tasks across diverse scenarios. Each task involves a specific user and a list of item candidates. Participants will need to extract the user's preferences and item information from datasets to generate personalized recommendations. This track explores the potential of LLM agents in recommendation tasks, offering new perspectives and application scenarios for advancing next-generation information retrieval and recommendation systems.

### Evaluation
In the Recommendation Track, participants are tasked with designing a recommendation agent that ranks the most likely item a user would select from a list of candidates. The ground truth for each task is generated using a predefined LLM agent that simulates user choices from a list of items based on the provided context and task query. The evaluation will measure how accurately the designed LLM agents rank the ground truth items. For each task, the model should rank a list of 20 candidate items, which will be then evaluated using the Top-N Hit Rate (with $N=1,3,5$).

Checkout [Recommendation Evaluation](https://github.com/tsinghua-fib-lab/WebSocietySimulator/blob/main/websocietysimulator/tools/evaluation_tool.py) for more calculation details.`;

        document.getElementById('content').innerHTML = marked.parse(markdownContent);
    </script>
</body>
</html> 